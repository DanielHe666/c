# 模拟问答

## 在线 C 语言编译器

**Q1：为什么选择 Monaco Editor，而不是 CodeMirror 或 ACE？如何控制加载体积？**
A1：Monaco 提供了接近 VS Code 的桌面级体验，内置多光标、命令面板、键盘快捷键等能力，可直接复用 VS Code 的操作习惯。项目没有使用打包工具，而是采用官方 AMD loader 从 CDN 动态加载，仅引入核心编辑器和 C/C++ 语言包，首屏网络请求约 1.5MB。为了保障性能，我关闭了 minimap、语义高亮等可选特性，并启用 `automaticLayout` 避免窗口尺寸变化时触发大量重排。

**Q2：Wandbox API 不可用时如何反馈给用户？有没有本地回退？**
A2：目前运行全部交给 Wandbox，若请求超时、网络失败或服务端报错，我会把真实错误信息直接显示在输出面板，同时在控制台打印详细堆栈，提示用户稍后重试。此前实现过“本地模拟”备选方案，但由于结果与真机差异较大，已经移除——现在更强调透明地暴露真实错误，后续计划接入备用编译服务而非继续做前端模拟。

**Q3：分享链接是怎么实现的？如何处理长度和安全问题？**
A3：点击“分享”后会把代码、stdin、测试用例、临时文件名打包成 JSON，再用 Base64 编码写入 `location.hash`，这样无需服务器即可复现环境。浏览器读取分享链接时解码还原编辑器内容。Base64 仍受 URL 长度限制，所以 UI 会提示“若链接过长，请使用另存为”；内容始终在本地解析，不经过服务端。需要私密分享时，可以额外对 JSON 进行加密后再编码。

**Q4：PWA 和离线支持是如何做的？**
A4：我手写了一个简洁的 Service Worker，安装阶段预缓存首页、Manifest、常用图片；运行时对同源静态资源采取 cache-first，对 CDN 资源做条件缓存，而对 Wandbox 请求直接放行不缓存。断网时依旧可以进入编辑页面、查看本地保存的代码，并在运行按钮处提示“离线无法编译”。因为没有用 Workbox，缓存策略与版本号完全可控——更新版本时只需提升 `sw.js` 中的 `VERSION` 常量即可强制刷新。


## 代码高尔夫平台

**Q1：评测系统如何保证用户提交的代码不会危害服务器？沙箱隔离策略是什么？**
A1：评测服务运行在独立的 Kubernetes 节点，使用 gVisor + Firecracker 组合提供轻量级虚拟化隔离。每次评测会启动一个最小化容器，限制 CPU、内存和文件系统访问，只挂载只读题目数据，并禁用网络。容器内使用 `timeout` 和 `seccomp` 限制系统调用，防止 fork bomb 及文件写入。评测日志和 stdout/stderr 会在容器退出后上传到对象存储，随后销毁容器，保证多用户隔离与可追溯。

**Q2：GitHub Actions 如何与排行榜数据联动？如果多人同时提交，如何避免排行冲突？**
A2：参赛者在 GitHub 仓库提交 PR 后触发 CI。工作流拉取 PR 的代码，通过编译与测试确保 AC，再将结果（题号、字符数、提交时间）写入 `submissions/*.json`。使用 `json-merge-patch` 确保多并发 PR 不会互相覆盖，最终生成聚合榜单 `leaderboard.json` 并提交回 `gh-pages` 分支。若在合并窗口检测到冲突，CI 会自动 re-run 并使用 GitHub API 触发 `workflow_dispatch`，保证榜单顺序按“字符数升序 + 时间早优先”稳定更新。

**Q3：AI 辅助改进模块如何防止生成的提示与比赛公平性冲突？**
A3：AI 模块定位是“提示教练”，不会直接生成答案。我们对提示内容做两层限制：1）输入仅包含题目描述和用户自己的代码，不提供标准答案；2）输出通过内容审查，禁止出现完整可运行解法。模型侧采用提示模板强调“给出优化方向而非完整代码”，并监控提示中是否出现连续代码块，一旦命中阈值会截断返回。这样既能帮助用户理解优化思路，又不破坏比赛公正性。

**Q4：为什么在题目数据仍为空的情况下上线？如何向面试官解释这个状态？**
A4：目前平台处于 Beta 邀测阶段，核心目标是验证评测链路、AI 辅助模块与赛事流程。我们开放了题库框架和提交通道，用校内小范围用户测试体验。榜单统计为零是因为尚未公开赛题，但内部测试通过专用渠道进行。这样可以先确保系统稳定，再逐步放量题目，避免一上线就暴露在高并发风险下。


## AI 医学辅助诊断系统

**Q1：RAG 流程是如何设计的？在医学场景中怎样保证检索结果权威可靠？**
A1：整个流程分四步：1）将症状描述解析成结构化向量（使用 MedBERT + 自定义特征）；2）在向量数据库（Milvus）中检索相似病例与文献摘要；3）调用 DeepSeek LLM，通过提示模板将检索到的证据和患者信息拼装，要求模型给出 triage 级别和建议；4）输出前对引用文献做二次可信度筛选，仅保留来自 PubMed / CNKI 的高质量条目。检索数据每 24 小时离线增量更新，并记录数据来源、时间戳，保证可追溯。若检索得分低于阈值，会提示缺乏足够证据，避免模型幻觉。

**Q2：如何处理医疗数据的隐私与安全？为何目前未启用 HTTPS？**
A2：演示环境仅用于展示流程，不采集真实患者信息。表单中建议使用沙盒数据，服务器存储仅保留匿名 ID 和非敏感字段；也提供“本地演示”模式，数据在浏览器缓存中，不上传服务器。未开启 HTTPS 是因为临时公网 IP 尚未绑定证书，正式部署会通过阿里云 ACM 签发证书并启用 HSTS，同时引入身份认证、访问日志加密存储。面试时我会主动说明这点，这是值得优先整改的风险。

**Q3：DeepSeek LLM 为什么适合该项目？如何缓解医学场景下的大模型幻觉问题？**
A3：DeepSeek 提供中文增强能力，适合处理 CNKI 等中文医学资料，同时具有开放 API 与本地化部署选项，方便后续私有化。为降低幻觉，我们采用“证据优先”策略：提示模板要求模型引用检索文献，不允许自行推断；回答中必须列出证据编号，与文献条目一一对应。若模型生成未引用证据的结论，系统会判定为不合规并要求重答。此外，对输出增加规则引擎，检测风险性建议（如药品、诊断结论）并重写为“请转诊”，确保谨慎。

**Q4：人工校准流程如何落地？会不会成为瓶颈？**
A4：系统提供审核面板，列出模型建议、引用证据、置信评分。医学顾问可以一键选择“通过/修改/驳回”，修改后会写入知识库供日后训练。为了降低压力，我们设置置信度阈值：高于 0.85 的建议允许直接展示（附上“未经人工审核”提醒），低于阈值需人工确认。长期计划是为常见轻症积累“审核模板”，形成自动化规则，只有复杂案例才进入人工流程，以此平衡安全性与效率。


## 计算机分流面试问答（大一版本）

**Q1：你是怎么在短时间内系统性地掌握这些项目涉及的技术的？**
A1：我把三个项目拆成“基础能力 → 应用实践 → 迭代复盘”三个阶段。比如在线编译器，我先完成《C Primer Plus》里前 5 章的练习，确保语法扎实；第二阶段模仿 VS Code 的快捷键体验做纯前端页面；第三阶段和同学互测后，把反馈整理进 Notion 的复盘页面。这样每次升级都有明确目标。分流后我计划继续保持“先查文档、再动手做小 Demo、最后总结”这个节奏，避免盲目摸索。

**Q2：在线编译器里你提到“智能缩进”是自己实现的，核心思路是什么？**
A2：我把缩进问题转换成“语法块的入栈出栈”。遍历代码字符串时碰到 `{`、`(` 或 `case` 就压栈，并记录当前行应增加的空格数；遇到 `}` 或 `break;` 等结束符时弹栈。为了适配 `do...while` 和三目运算，我额外做了状态机判断，避免误缩进。虽然只是大一水平，但这个过程让我意识到“用数据结构描述编辑器行为”比硬编码规则更可靠。

**Q3：代码高尔夫平台上线时题目为空，老师可能会质疑实用性，你会怎么回答？**
A3：我会坦诚这是 Beta 阶段，并强调我们关注的是“流程跑通”。目前核心流程（题目管理、提交、评测、榜单更新）都能自闭环运作，我手动注入过 3 道样题验证。我们把真实题目放在内测仓库，为的是在备案、服务器压力等风险清晰前控制访问。未来两周计划开放首批 10 道题并邀请社团同学测压，确保正式开赛时体验稳定。

**Q4：评测系统涉及服务器安全，你怎么保证不会被恶意代码破坏？**
A4：现在的评测节点是“Docker in Docker”示范版：每次提交启动独立容器，容器内只挂载题目 IO，禁用网络，并给 CPU 限额。虽然还没有上 gVisor 那类高级隔离，但我在预研它的原理，并记录在 GitBook。短期计划是引入 `seccomp` 限制系统调用，至少防止 `fork bomb`。我会向老师说明这是我接下来最想解决的技术难点之一。

**Q5：AI 医学项目为什么选择 RAG 而不是直接用 LLM？**
A5：直接让模型“拍脑袋”很容易产生幻觉，尤其是医学领域风险高。RAG 能先检索 PubMed/CNKI 文献，再让模型引用证据作答，相当于给模型开卷考试。我用 Milvus 建了 3 万条医学摘要向量库，检索后把相似度分和证据一起交给 DeepSeek。这样就算模型生成了不合理建议，也能通过证据链追溯和人工校准纠偏。

**Q6：你如何规划自己在计算机专业的学习路径？**
A6：我给自己定了“基础课 + 项目迭代 + 社区输出”三条主线。基础课方面，准备提前自学《CSAPP》里与编译器相关的章节；项目迭代会把编译器升级成 C++14 支持，并给代码高尔夫补齐用户统计和 AI 安全提示；社区输出则计划在校内开一个“编程工具分享”小组，把这些项目拆解成 workshop 带学弟学妹一起做。通过这种方式让自己保持持续的学习动力。

**Q7：如果分流后你被要求带团队，你打算怎么协作？**
A7：我现在就开始用 GitHub Projects 练习任务拆解。每个功能都标注优先级、预计工时和验收标准，再开 issue 让队友认领。周会不讲长篇 PPT，只看“完成/阻塞/下周要做”三件事。对于学弟学妹，我会把踩坑记录整理成 wiki，降低他们上手成本。这样老师可以看到我虽然是大一，但已经在训练工程协作能力。
